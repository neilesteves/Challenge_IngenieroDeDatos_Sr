{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proceso para convertir datos de CSV a formato Parquet usando Pyspark"
      ],
      "metadata": {
        "id": "AiOZPtCMu63T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalamos la librería pyspark"
      ],
      "metadata": {
        "id": "2dIMfsepu_AO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ELZ3N64ydh1",
        "outputId": "8447870e-cbff-4e17-8d31-0997f2025d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de los archivos"
      ],
      "metadata": {
        "id": "8lykC0nkG6h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subir los archivos a la ruta \"/content/kaggle\":\n",
        "\n",
        "\n",
        "*   /content/kaggle/country_wise_latest.csv\n",
        "*   /content/kaggle/covid_19_clean_complete.csv\n",
        "*   /content/kaggle/day_wise.csv\n",
        "*   /content/kaggle/full_grouped.csv\n",
        "*   /content/kaggle/usa_county_wise.csv\n",
        "*   /content/kaggle/worldometer_data.csv\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i6-XIHMOGVLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importamos la librerías"
      ],
      "metadata": {
        "id": "W3DNFAsOuiTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
        "import csv"
      ],
      "metadata": {
        "id": "stxhre_bukdE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos la Clase CSVtoParquet"
      ],
      "metadata": {
        "id": "8RNt3DvHw5nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase \"CSVtoParquet\" realizará la lectura de los archivos \".csv\" y los escribirá en formato parquet, considerando el tipo de cada campo"
      ],
      "metadata": {
        "id": "ZBZdTlNCw8xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CSVtoParquet:\n",
        "    def __init__(self, sc, spark):\n",
        "        self.sc = sc\n",
        "        self.spark = spark\n",
        "\n",
        "    # Se define la función convert_country_csv para leer el archivo \"country_wise_latest.csv\" y escribirlo en formato parquet\n",
        "    def convert_country_csv(self, file_path, output_path):\n",
        "        try:\n",
        "          # Leer CSV haciendo uso de RDD\n",
        "          csv_rdd = self.sc.textFile(file_path)\n",
        "\n",
        "          # Parsea RDD dentro de una lista de tuplas con tipos de datos específicos\n",
        "          header = csv_rdd.first()\n",
        "          data = csv_rdd.filter(lambda x: x != header).map(lambda x: x.split(\",\"))\n",
        "          schema = StructType([\n",
        "              StructField('country_region', StringType(), True),\n",
        "              StructField('confirmed', IntegerType(), True),\n",
        "              StructField('deaths', IntegerType(), True),\n",
        "              StructField('recovered', IntegerType(), True),\n",
        "              StructField('active', IntegerType(), True),\n",
        "              StructField('new_cases', IntegerType(), True),\n",
        "              StructField('new_deaths', IntegerType(), True),\n",
        "              StructField('new_recovered', IntegerType(), True),\n",
        "              StructField('deaths_100_cases', DoubleType(), True),\n",
        "              StructField('recovered_100_cases', DoubleType(), True),\n",
        "              StructField('deaths_100_recovered', DoubleType(), True),\n",
        "              StructField('confirmed_last_week', IntegerType(), True),\n",
        "              StructField('week_change', IntegerType(), True),\n",
        "              StructField('week_perc_increase', DoubleType(), True),\n",
        "              StructField('who_region', StringType(), True),\n",
        "          ])\n",
        "          data = data.map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), int(x[4]), int(x[5]), int(x[6]), int(x[7]), float(x[8]), float(x[9]), float(x[10]), int(x[11]), int(x[12]), float(x[13]), x[14]))\n",
        "\n",
        "          # Convertir RDD a DataFrame con un esquema específico\n",
        "          df = self.spark.createDataFrame(data, schema)\n",
        "\n",
        "          df.show(5)\n",
        "          df.printSchema()\n",
        "\n",
        "          # Escribir DataFrame a formato Parquet\n",
        "          df.write.format(\"parquet\").mode(\"overwrite\").save(output_path)\n",
        "          print(\"\\n\", \"Se generó en formato parquet en:\",output_path)\n",
        "        except Exception as error:\n",
        "          print(\"Ocurrió un error:\", error)\n",
        "        \n",
        "    # Se define la función convert_covid_csv para leer el archivo \"covid_19_clean_complete.csv\" y escribirlo en formato parquet\n",
        "    def convert_covid_csv(self, file_path, output_path):\n",
        "        try:\n",
        "          # Leer CSV haciendo uso de RDD\n",
        "          csv_rdd = self.sc.textFile(file_path)\n",
        "\n",
        "          # Parsea RDD dentro de una lista de tuplas con tipos de datos específicos\n",
        "          header = csv_rdd.first()\n",
        "          data = csv_rdd.filter(lambda x: x != header).map(lambda x: x.split(\",\"))\n",
        "          schema = StructType([\n",
        "              StructField('province_state', StringType(), True),\n",
        "              StructField('country_region', StringType(), True),\n",
        "              StructField('lat', DoubleType(), True),\n",
        "              StructField('long', DoubleType(), True),\n",
        "              StructField('date', StringType(), True),\n",
        "              StructField('confirmed', IntegerType(), True),\n",
        "              StructField('deaths', IntegerType(), True),\n",
        "              StructField('recovered', IntegerType(), True),\n",
        "              StructField('active', IntegerType(), True),\n",
        "              StructField('who_region', StringType(), True),\n",
        "          ])\n",
        "          data = data.map(lambda x: (x[0], x[1], float(x[2]), float(x[3]), x[4], int(x[5]), int(x[6]), int(x[7]), int(x[8]), x[9]))\n",
        "\n",
        "          # Convertir RDD a DataFrame con un esquema específico\n",
        "          df = self.spark.createDataFrame(data, schema)\n",
        "\n",
        "          df.show(5)\n",
        "          df.printSchema()\n",
        "          # Escribir DataFrame a formato Parquet\n",
        "          df.write.format(\"parquet\").mode(\"overwrite\").save(output_path)\n",
        "          print(\"\\n\", \"Se generó en formato parquet en:\",output_path)\n",
        "        except Exception as error:\n",
        "          print(\"Ocurrió un error:\", error)\n",
        "      \n",
        "    # Se define la función convert_day_wise_csv para leer el archivo \"day_wise.csv\" y escribirlo en formato parquet\n",
        "    def convert_day_wise_csv(self, file_path, output_path):\n",
        "        try:\n",
        "          # Leer CSV haciendo uso de RDD\n",
        "          csv_rdd = self.sc.textFile(file_path)\n",
        "\n",
        "          # Parsea RDD dentro de una lista de tuplas con tipos de datos específicos\n",
        "          header = csv_rdd.first()\n",
        "          data = csv_rdd.filter(lambda x: x != header).map(lambda x: x.split(\",\"))\n",
        "          schema = StructType([\n",
        "              StructField('date', StringType(), True),\n",
        "              StructField('confirmed', IntegerType(), True),\n",
        "              StructField('deaths', IntegerType(), True),\n",
        "              StructField('recovered', IntegerType(), True),\n",
        "              StructField('active', IntegerType(), True),\n",
        "              StructField('new_cases', IntegerType(), True),\n",
        "              StructField('new_deaths', IntegerType(), True),\n",
        "              StructField('new_recovered', IntegerType(), True),\n",
        "              StructField('deaths_100_cases', DoubleType(), True),\n",
        "              StructField('recovered_100_cases', DoubleType(), True),\n",
        "              StructField('deaths_100_recovered', DoubleType(), True),\n",
        "              StructField('nro_countries', IntegerType(), True),\n",
        "          ])\n",
        "          data = data.map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), int(x[4]), int(x[5]), int(x[6]), int(x[7]), float(x[8]), float(x[9]), float(x[10]), int(x[11])))\n",
        "\n",
        "          # Convertir RDD a DataFrame con un esquema específico\n",
        "          df = self.spark.createDataFrame(data, schema)\n",
        "\n",
        "          df.show(5)\n",
        "          df.printSchema()\n",
        "          # Escribir DataFrame a formato Parquet\n",
        "          df.write.format(\"parquet\").mode(\"overwrite\").save(output_path)\n",
        "          print(\"\\n\", \"Se generó en formato parquet en:\",output_path)\n",
        "        except Exception as error:\n",
        "          print(\"Ocurrió un error:\", error)\n",
        "\n",
        "    # Se define la función convert_full_grouped_csv para leer el archivo \"full_grouped.csv\" y escribirlo en formato parquet\n",
        "    def convert_full_grouped_csv(self, file_path, output_path):\n",
        "        try:\n",
        "          # Leer CSV haciendo uso de RDD\n",
        "          csv_rdd = self.sc.textFile(file_path)\n",
        "\n",
        "          # Parsea RDD dentro de una lista de tuplas con tipos de datos específicos\n",
        "          header = csv_rdd.first()\n",
        "          data = csv_rdd.filter(lambda x: x != header).map(lambda x: x.split(\",\"))\n",
        "          schema = StructType([\n",
        "              StructField('date', StringType(), True),\n",
        "              StructField('country_region', StringType(), True),\n",
        "              StructField('confirmed', IntegerType(), True),\n",
        "              StructField('deaths', IntegerType(), True),\n",
        "              StructField('recovered', IntegerType(), True),\n",
        "              StructField('active', IntegerType(), True),\n",
        "              StructField('new_cases', IntegerType(), True),\n",
        "              StructField('new_deaths', IntegerType(), True),\n",
        "              StructField('new_recovered', IntegerType(), True),\n",
        "              StructField('who_region', StringType(), True),\n",
        "          ])\n",
        "          data = data.map(lambda x: (x[0], x[1], int(x[2]), int(x[3]), int(x[4]), int(x[5]), int(x[6]), int(x[7]), int(x[8]), x[9]))\n",
        "\n",
        "          # Convertir RDD a DataFrame con un esquema específico\n",
        "          df = self.spark.createDataFrame(data, schema)\n",
        "\n",
        "          df.show(5)\n",
        "          df.printSchema()\n",
        "          # Escribir DataFrame a formato Parquet\n",
        "          df.write.format(\"parquet\").mode(\"overwrite\").save(output_path)\n",
        "          print(\"\\n\", \"Se generó en formato parquet en:\",output_path)\n",
        "        except Exception as error:\n",
        "          print(\"Ocurrió un error:\", error)\n",
        "\n",
        "    # Se define la función convert_usa_county_csv para leer el archivo \"usa_county_wise.csv\" y escribirlo en formato parquet\n",
        "    def convert_usa_county_csv(self, file_path, output_path):\n",
        "        try:\n",
        "          # Leer CSV haciendo uso de RDD\n",
        "          csv_rdd = self.sc.textFile(file_path)\n",
        "\n",
        "          # Parsea RDD dentro de una lista de tuplas con tipos de datos específicos\n",
        "          header = csv_rdd.first()\n",
        "          data = csv_rdd.filter(lambda x: x != header).map(lambda linea: next(csv.reader([linea])))\n",
        "          schema = StructType([\n",
        "              StructField('uid', IntegerType(), True),\n",
        "              StructField('iso2', StringType(), True),\n",
        "              StructField('iso3', StringType(), True),\n",
        "              StructField('code3', IntegerType(), True),\n",
        "              StructField('fips', DoubleType(), True),\n",
        "              StructField('admin2', StringType(), True),\n",
        "              StructField('province_state', StringType(), True),\n",
        "              StructField('country_region', StringType(), True),\n",
        "              StructField('lat', DoubleType(), True),\n",
        "              StructField('long', DoubleType(), True),\n",
        "              StructField('combined_key', StringType(), True),\n",
        "              StructField('date', StringType(), True),\n",
        "              StructField('confirmed', IntegerType(), True),\n",
        "              StructField('deaths', IntegerType(), True),\n",
        "          ])\n",
        "          data = data.map(lambda x: (int(x[0]), x[1], x[2], int(x[3]), float(x[4]) if(x[4] != '') else 0.0 , x[5], x[6], x[7], float(x[8]), float(x[9]), x[10], x[11], int(x[12]), int(x[13])))\n",
        "\n",
        "          # Convertir RDD a DataFrame con un esquema específico\n",
        "          df = self.spark.createDataFrame(data, schema)\n",
        "\n",
        "          df.show(5)\n",
        "          df.printSchema()\n",
        "          # Escribir DataFrame a formato Parquet\n",
        "          df.write.format(\"parquet\").mode(\"overwrite\").save(output_path)\n",
        "          print(\"\\n\", \"Se generó en formato parquet en:\",output_path)\n",
        "        except Exception as error:\n",
        "          print(\"Ocurrió un error:\", error)\n",
        "\n",
        "    # Se define la función convert_worldometer_csv para leer el archivo \"worldometer_data.csv\" y escribirlo en formato parquet\n",
        "    def convert_worldometer_csv(self, file_path, output_path):\n",
        "        try:\n",
        "          # Leer CSV haciendo uso de RDD\n",
        "          csv_rdd = self.sc.textFile(file_path)\n",
        "\n",
        "          # Parsea RDD dentro de una lista de tuplas con tipos de datos específicos\n",
        "          header = csv_rdd.first()\n",
        "          data = csv_rdd.filter(lambda x: x != header).map(lambda linea: next(csv.reader([linea])))\n",
        "          schema = StructType([\n",
        "              StructField('country_region', StringType(), True),\n",
        "              StructField('continent', StringType(), True),\n",
        "              StructField('population', IntegerType(), True),\n",
        "              StructField('totalcases', IntegerType(), True),\n",
        "              StructField('newcases', IntegerType(), True),\n",
        "              StructField('totaldeaths', IntegerType(), True),\n",
        "              StructField('newdeaths', IntegerType(), True),\n",
        "              StructField('totalrecovered', IntegerType(), True),\n",
        "              StructField('newrecovered', IntegerType(), True),\n",
        "              StructField('activecases', IntegerType(), True),\n",
        "              StructField('serious_critical', IntegerType(), True),\n",
        "              StructField('tot_cases_1m_pop', IntegerType(), True),\n",
        "              StructField('deaths_1m_pop', DoubleType(), True),\n",
        "              StructField('totaltests', IntegerType(), True),\n",
        "              StructField('tests_1m_pop', IntegerType(), True),\n",
        "              StructField('who_region', StringType(), True),\n",
        "          ])\n",
        "          data = data.map(lambda x: (x[0], x[1], cast_string_to_int(x[2]), cast_string_to_int(x[3]), cast_string_to_int(x[4]), cast_string_to_int(x[5]), cast_string_to_int(x[6]), cast_string_to_int(x[7]), cast_string_to_int(x[8]), cast_string_to_int(x[9]), cast_string_to_int(x[10]), cast_string_to_int(x[11]), cast_string_to_float(x[12]), cast_string_to_int(x[13]), cast_string_to_int(x[14]), x[15]))\n",
        "\n",
        "          # Convertir RDD a DataFrame con un esquema específico\n",
        "          df = self.spark.createDataFrame(data, schema)\n",
        "\n",
        "          df.show(5)\n",
        "          df.printSchema()\n",
        "          # Escribir DataFrame a formato Parquet\n",
        "          df.write.format(\"parquet\").mode(\"overwrite\").save(output_path)\n",
        "          print(\"\\n\", \"Se generó en formato parquet en:\",output_path)\n",
        "        except Exception as error:\n",
        "          print(\"Ocurrió un error:\", error)\n"
      ],
      "metadata": {
        "id": "Gf4mSAe62nV-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define la función cast_string_to_int para realizar el casteo de datos String a entero\n",
        "def cast_string_to_int(string):\n",
        "  if(string != ''):\n",
        "    return int(string)\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "nPcWM4uVuot_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define la función cast_string_to_int para realizar el casteo de datos String a decimales\n",
        "def cast_string_to_float(string):\n",
        "  if(string != ''):\n",
        "    return float(string)\n",
        "  else:\n",
        "    return 0.0"
      ],
      "metadata": {
        "id": "yU_8KJfzuwnL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se especifica la configuración\n",
        "conf = SparkConf().setAppName(\"CSV to Parquet\")"
      ],
      "metadata": {
        "id": "lrx-dXdpur5K"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se instancia sparkContext con la configuración especificada\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "ywLhKJmwFdRf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea la sessión de spark\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "mHr7-PnAFdTj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intancia y ejecución de la clase CSVtoParquet"
      ],
      "metadata": {
        "id": "35F4culiH2K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se instancia la clase\n",
        "csv_to_parquet = CSVtoParquet(sc,spark)"
      ],
      "metadata": {
        "id": "qIXhD7kRFUYv"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecuta el método que convierte el archivo \"country_wise_latest.csv\" en formato parquet\n",
        "csv_to_parquet.convert_country_csv(\"/content/kaggle/country_wise_latest.csv\", \"/content/parquet/parquet_country_wise_latest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtaS-zw8FnP3",
        "outputId": "7f6de0b0-487a-4d3d-db3c-14db1d6eb4fd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------+------+---------+------+---------+----------+-------------+----------------+-------------------+--------------------+-------------------+-----------+------------------+--------------------+\n",
            "|country_region|confirmed|deaths|recovered|active|new_cases|new_deaths|new_recovered|deaths_100_cases|recovered_100_cases|deaths_100_recovered|confirmed_last_week|week_change|week_perc_increase|          who_region|\n",
            "+--------------+---------+------+---------+------+---------+----------+-------------+----------------+-------------------+--------------------+-------------------+-----------+------------------+--------------------+\n",
            "|   Afghanistan|    36263|  1269|    25198|  9796|      106|        10|           18|             3.5|              69.49|                5.04|              35526|        737|              2.07|Eastern Mediterra...|\n",
            "|       Albania|     4880|   144|     2745|  1991|      117|         6|           63|            2.95|              56.25|                5.25|               4171|        709|              17.0|              Europe|\n",
            "|       Algeria|    27973|  1163|    18837|  7973|      616|         8|          749|            4.16|              67.34|                6.17|              23691|       4282|             18.07|              Africa|\n",
            "|       Andorra|      907|    52|      803|    52|       10|         0|            0|            5.73|              88.53|                6.48|                884|         23|               2.6|              Europe|\n",
            "|        Angola|      950|    41|      242|   667|       18|         1|            0|            4.32|              25.47|               16.94|                749|        201|             26.84|              Africa|\n",
            "+--------------+---------+------+---------+------+---------+----------+-------------+----------------+-------------------+--------------------+-------------------+-----------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- country_region: string (nullable = true)\n",
            " |-- confirmed: integer (nullable = true)\n",
            " |-- deaths: integer (nullable = true)\n",
            " |-- recovered: integer (nullable = true)\n",
            " |-- active: integer (nullable = true)\n",
            " |-- new_cases: integer (nullable = true)\n",
            " |-- new_deaths: integer (nullable = true)\n",
            " |-- new_recovered: integer (nullable = true)\n",
            " |-- deaths_100_cases: double (nullable = true)\n",
            " |-- recovered_100_cases: double (nullable = true)\n",
            " |-- deaths_100_recovered: double (nullable = true)\n",
            " |-- confirmed_last_week: integer (nullable = true)\n",
            " |-- week_change: integer (nullable = true)\n",
            " |-- week_perc_increase: double (nullable = true)\n",
            " |-- who_region: string (nullable = true)\n",
            "\n",
            "\n",
            " Se generó en formato parquet en: /content/parquet/parquet_country_wise_latest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecuta el método que convierte el archivo \"covid_19_clean_complete.csv\" en formato parquet\n",
        "csv_to_parquet.convert_covid_csv(\"/content/kaggle/covid_19_clean_complete.csv\", \"/content/parquet/parquet_covid_19_clean_complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTF0wdBiFqWx",
        "outputId": "8825e57b-8209-47b6-e078-245f9334845f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+--------+---------+----------+---------+------+---------+------+--------------------+\n",
            "|province_state|country_region|     lat|     long|      date|confirmed|deaths|recovered|active|          who_region|\n",
            "+--------------+--------------+--------+---------+----------+---------+------+---------+------+--------------------+\n",
            "|              |   Afghanistan|33.93911|67.709953|2020-01-22|        0|     0|        0|     0|Eastern Mediterra...|\n",
            "|              |       Albania| 41.1533|  20.1683|2020-01-22|        0|     0|        0|     0|              Europe|\n",
            "|              |       Algeria| 28.0339|   1.6596|2020-01-22|        0|     0|        0|     0|              Africa|\n",
            "|              |       Andorra| 42.5063|   1.5218|2020-01-22|        0|     0|        0|     0|              Europe|\n",
            "|              |        Angola|-11.2027|  17.8739|2020-01-22|        0|     0|        0|     0|              Africa|\n",
            "+--------------+--------------+--------+---------+----------+---------+------+---------+------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- province_state: string (nullable = true)\n",
            " |-- country_region: string (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- long: double (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- confirmed: integer (nullable = true)\n",
            " |-- deaths: integer (nullable = true)\n",
            " |-- recovered: integer (nullable = true)\n",
            " |-- active: integer (nullable = true)\n",
            " |-- who_region: string (nullable = true)\n",
            "\n",
            "\n",
            " Se generó en formato parquet en: /content/parquet/parquet_covid_19_clean_complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecuta el método que convierte el archivo \"day_wise.csv\" en formato parquet\n",
        "csv_to_parquet.convert_day_wise_csv(\"/content/kaggle/day_wise.csv\", \"/content/parquet/parquet_day_wise\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U8OpCNuFqbk",
        "outputId": "5e856475-e480-4026-a925-dd3e5de73388"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------+---------+------+---------+----------+-------------+----------------+-------------------+--------------------+-------------+\n",
            "|      date|confirmed|deaths|recovered|active|new_cases|new_deaths|new_recovered|deaths_100_cases|recovered_100_cases|deaths_100_recovered|nro_countries|\n",
            "+----------+---------+------+---------+------+---------+----------+-------------+----------------+-------------------+--------------------+-------------+\n",
            "|2020-01-22|      555|    17|       28|   510|        0|         0|            0|            3.06|               5.05|               60.71|            6|\n",
            "|2020-01-23|      654|    18|       30|   606|       99|         1|            2|            2.75|               4.59|                60.0|            8|\n",
            "|2020-01-24|      941|    26|       36|   879|      287|         8|            6|            2.76|               3.83|               72.22|            9|\n",
            "|2020-01-25|     1434|    42|       39|  1353|      493|        16|            3|            2.93|               2.72|              107.69|           11|\n",
            "|2020-01-26|     2118|    56|       52|  2010|      684|        14|           13|            2.64|               2.46|              107.69|           13|\n",
            "+----------+---------+------+---------+------+---------+----------+-------------+----------------+-------------------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- confirmed: integer (nullable = true)\n",
            " |-- deaths: integer (nullable = true)\n",
            " |-- recovered: integer (nullable = true)\n",
            " |-- active: integer (nullable = true)\n",
            " |-- new_cases: integer (nullable = true)\n",
            " |-- new_deaths: integer (nullable = true)\n",
            " |-- new_recovered: integer (nullable = true)\n",
            " |-- deaths_100_cases: double (nullable = true)\n",
            " |-- recovered_100_cases: double (nullable = true)\n",
            " |-- deaths_100_recovered: double (nullable = true)\n",
            " |-- nro_countries: integer (nullable = true)\n",
            "\n",
            "\n",
            " Se generó en formato parquet en: /content/parquet/parquet_day_wise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecuta el método que convierte el archivo \"full_grouped.csv\" en formato parquet\n",
        "csv_to_parquet.convert_full_grouped_csv(\"/content/kaggle/full_grouped.csv\", \"/content/parquet/parquet_full_grouped\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5nquYmwFqjZ",
        "outputId": "07acc8e5-2c1e-490c-8a0a-6decab85e077"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+---------+------+---------+------+---------+----------+-------------+--------------------+\n",
            "|      date|country_region|confirmed|deaths|recovered|active|new_cases|new_deaths|new_recovered|          who_region|\n",
            "+----------+--------------+---------+------+---------+------+---------+----------+-------------+--------------------+\n",
            "|2020-01-22|   Afghanistan|        0|     0|        0|     0|        0|         0|            0|Eastern Mediterra...|\n",
            "|2020-01-22|       Albania|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
            "|2020-01-22|       Algeria|        0|     0|        0|     0|        0|         0|            0|              Africa|\n",
            "|2020-01-22|       Andorra|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
            "|2020-01-22|        Angola|        0|     0|        0|     0|        0|         0|            0|              Africa|\n",
            "+----------+--------------+---------+------+---------+------+---------+----------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- country_region: string (nullable = true)\n",
            " |-- confirmed: integer (nullable = true)\n",
            " |-- deaths: integer (nullable = true)\n",
            " |-- recovered: integer (nullable = true)\n",
            " |-- active: integer (nullable = true)\n",
            " |-- new_cases: integer (nullable = true)\n",
            " |-- new_deaths: integer (nullable = true)\n",
            " |-- new_recovered: integer (nullable = true)\n",
            " |-- who_region: string (nullable = true)\n",
            "\n",
            "\n",
            " Se generó en formato parquet en: /content/parquet/parquet_full_grouped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecuta el método que convierte el archivo \"usa_county_wise.csv\" en formato parquet\n",
        "csv_to_parquet.convert_usa_county_csv(\"/content/kaggle/usa_county_wise.csv\", \"/content/parquet/parquet_usa_county_wise\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IUlf_3EHfyF",
        "outputId": "b330e3c9-b68b-4a90-aed3-7ba06c416073"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
            "|     uid|iso2|iso3|code3|   fips|  admin2|      province_state|country_region|                lat|              long|        combined_key|   date|confirmed|deaths|\n",
            "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
            "|      16|  AS| ASM|   16|   60.0|        |      American Samoa|            US|-14.270999999999999|          -170.132|  American Samoa, US|1/22/20|        0|     0|\n",
            "|     316|  GU| GUM|  316|   66.0|        |                Guam|            US|            13.4443|          144.7937|            Guam, US|1/22/20|        0|     0|\n",
            "|     580|  MP| MNP|  580|   69.0|        |Northern Mariana ...|            US|            15.0979|          145.6739|Northern Mariana ...|1/22/20|        0|     0|\n",
            "|63072001|  PR| PRI|  630|72001.0|Adjuntas|         Puerto Rico|            US| 18.180117000000006|        -66.754367|Adjuntas, Puerto ...|1/22/20|        0|     0|\n",
            "|63072003|  PR| PRI|  630|72003.0|  Aguada|         Puerto Rico|            US|          18.360255|-67.17513100000001|Aguada, Puerto Ri...|1/22/20|        0|     0|\n",
            "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- uid: integer (nullable = true)\n",
            " |-- iso2: string (nullable = true)\n",
            " |-- iso3: string (nullable = true)\n",
            " |-- code3: integer (nullable = true)\n",
            " |-- fips: double (nullable = true)\n",
            " |-- admin2: string (nullable = true)\n",
            " |-- province_state: string (nullable = true)\n",
            " |-- country_region: string (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- long: double (nullable = true)\n",
            " |-- combined_key: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- confirmed: integer (nullable = true)\n",
            " |-- deaths: integer (nullable = true)\n",
            "\n",
            "\n",
            " Se generó en formato parquet en: /content/parquet/parquet_usa_county_wise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecuta el método que convierte el archivo \"worldometer_data.csv\" en formato parquet\n",
        "csv_to_parquet.convert_worldometer_csv(\"/content/kaggle/worldometer_data.csv\", \"/content/parquet/parquet_worldometer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4TjCMlWHf09",
        "outputId": "adb8d253-49d1-47ea-c1b8-66932a6b2d89"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------+\n",
            "|country_region|    continent|population|totalcases|newcases|totaldeaths|newdeaths|totalrecovered|newrecovered|activecases|serious_critical|tot_cases_1m_pop|deaths_1m_pop|totaltests|tests_1m_pop|    who_region|\n",
            "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------+\n",
            "|           USA|North America| 331198130|   5032179|       0|     162804|        0|       2576668|           0|    2292707|           18296|           15194|        492.0|  63139605|      190640|      Americas|\n",
            "|        Brazil|South America| 212710692|   2917562|       0|      98644|        0|       2047660|           0|     771258|            8318|           13716|        464.0|  13206188|       62085|      Americas|\n",
            "|         India|         Asia|1381344997|   2025409|       0|      41638|        0|       1377384|           0|     606387|            8944|            1466|         30.0|  22149351|       16035|South-EastAsia|\n",
            "|        Russia|       Europe| 145940924|    871894|       0|      14606|        0|        676357|           0|     180931|            2300|            5974|        100.0|  29716907|      203623|        Europe|\n",
            "|  South Africa|       Africa|  59381566|    538184|       0|       9604|        0|        387316|           0|     141264|             539|            9063|        162.0|   3149807|       53044|        Africa|\n",
            "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- country_region: string (nullable = true)\n",
            " |-- continent: string (nullable = true)\n",
            " |-- population: integer (nullable = true)\n",
            " |-- totalcases: integer (nullable = true)\n",
            " |-- newcases: integer (nullable = true)\n",
            " |-- totaldeaths: integer (nullable = true)\n",
            " |-- newdeaths: integer (nullable = true)\n",
            " |-- totalrecovered: integer (nullable = true)\n",
            " |-- newrecovered: integer (nullable = true)\n",
            " |-- activecases: integer (nullable = true)\n",
            " |-- serious_critical: integer (nullable = true)\n",
            " |-- tot_cases_1m_pop: integer (nullable = true)\n",
            " |-- deaths_1m_pop: double (nullable = true)\n",
            " |-- totaltests: integer (nullable = true)\n",
            " |-- tests_1m_pop: integer (nullable = true)\n",
            " |-- who_region: string (nullable = true)\n",
            "\n",
            "\n",
            " Se generó en formato parquet en: /content/parquet/parquet_worldometer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finaliza la sessión de spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "MFKo0VT1Fqlv"
      },
      "execution_count": 52,
      "outputs": []
    }
  ]
}