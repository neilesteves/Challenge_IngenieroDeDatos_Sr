{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proceso para convertir datos de CSV a formato Parquet usando Pandas"
      ],
      "metadata": {
        "id": "tHVBat0Ng7VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalamos la librería pandas"
      ],
      "metadata": {
        "id": "u2uS7E6mhSzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYP_Cw73t27P",
        "outputId": "10694ec5-a90c-4def-8cb1-89c788678491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de los archivos"
      ],
      "metadata": {
        "id": "uWP6ONE4HE5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subir los archivos a la ruta \"/content/kaggle\":\n",
        "\n",
        "\n",
        "*   /content/kaggle/country_wise_latest.csv\n",
        "*   /content/kaggle/covid_19_clean_complete.csv\n",
        "*   /content/kaggle/day_wise.csv\n",
        "*   /content/kaggle/full_grouped.csv\n",
        "*   /content/kaggle/usa_county_wise.csv\n",
        "*   /content/kaggle/worldometer_data.csv\n",
        "\n",
        "Además, crear la ruta \"/content/parquet_pandas\" para almacenar los archivos parquet\n"
      ],
      "metadata": {
        "id": "uMvmrH_gHE7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importamos la librería pandas"
      ],
      "metadata": {
        "id": "e-f70VbahpcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "k6RssopbhntU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos la Clase CountryToParquet"
      ],
      "metadata": {
        "id": "loaeuJJ-gvMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase \"CountryToParquet\" realizará la lectura del archivo \"country_wise_latest.csv\" y lo escribira en formato parquet, considerando el tipo de cada campo"
      ],
      "metadata": {
        "id": "DkFU6oTkh4Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define la clase CountryToParquet con un constructor que recibe los nombres de archivo de entrada y salida.\n",
        "class CountryToParquet:\n",
        "    def __init__(self, input_file, output_file):\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "    def read_csv(self):\n",
        "        # Se define un diccionario dtype que especifica los tipos de campo para leer el archivo CSV. \n",
        "        dtype = {\n",
        "            'country_region': str,\n",
        "            'confirmed': int,\n",
        "            'deaths': int,\n",
        "            'recovered': int,\n",
        "            'active': int,\n",
        "            'new_cases': int,\n",
        "            'new_deaths': int,\n",
        "            'new_recovered': int,\n",
        "            'deaths_100_cases': float,\n",
        "            'recovered_100_cases': float,\n",
        "            'deaths_100_recovered': float,\n",
        "            'confirmed_last_week': int,\n",
        "            'week_change': int,\n",
        "            'week_perc_increase': float,\n",
        "            'who_region': str,\n",
        "        }\n",
        "        # Se crea una lista columns que contiene los nombres de las columnas del DataFrame. \n",
        "        columns = ['country_region','confirmed','deaths','recovered','active','new_cases','new_deaths','new_recovered','deaths_100_cases','recovered_100_cases','deaths_100_recovered','confirmed_last_week','week_change','week_perc_increase','who_region']\n",
        "\n",
        "        #lee el archivo CSV, se proporciona los nombres de columna personalizados (names=columns), omitiendo la primera fila (encabezado) del archivo y aplicando los tipos de campo especificados. \n",
        "        df = pd.read_csv(self.input_file,header=None, names=columns, skiprows=1, dtype=dtype)\n",
        "        return df\n",
        "    # Se define un método convert_to_parquet() que toma un DataFrame df como entrada y escribe el DataFrame en formato Parquet.\n",
        "    def convert_to_parquet(self, df):\n",
        "        # Escribir el DataFrame en formato Parquet\n",
        "        df.to_parquet(self.output_file)\n",
        "        print(\"\\nSe generó el archivo \", self.output_file)\n",
        "\n",
        "    #realiza el proceso completo de conversión. obtiene el dataframe del método read_csv, muestra los tipos de datos de las columnas y las primeras 5 filas del DataFrame, y luego llama al método convert_to_parquet() para convertir el DataFrame a formato Parquet.\n",
        "    def convert(self):\n",
        "        df = self.read_csv()\n",
        "        print(df.dtypes, '\\n')\n",
        "        print(df.head(5))\n",
        "        self.convert_to_parquet(df)\n"
      ],
      "metadata": {
        "id": "EWXSHzf-uAVT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea la instancia del convertidor y realizar la conversión"
      ],
      "metadata": {
        "id": "BPtuVfqaiTBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country = CountryToParquet(\"/content/kaggle/country_wise_latest.csv\", \"/content/parquet_pandas/country_wise_latest.parquet\")\n",
        "country.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rem-rNQ4iNaD",
        "outputId": "487b3cd0-c4be-4a43-f95a-86aa6b84f63a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "country_region           object\n",
            "confirmed                 int64\n",
            "deaths                    int64\n",
            "recovered                 int64\n",
            "active                    int64\n",
            "new_cases                 int64\n",
            "new_deaths                int64\n",
            "new_recovered             int64\n",
            "deaths_100_cases        float64\n",
            "recovered_100_cases     float64\n",
            "deaths_100_recovered    float64\n",
            "confirmed_last_week       int64\n",
            "week_change               int64\n",
            "week_perc_increase      float64\n",
            "who_region               object\n",
            "dtype: object \n",
            "\n",
            "  country_region  confirmed  deaths  recovered  active  new_cases  new_deaths  \\\n",
            "0    Afghanistan      36263    1269      25198    9796        106          10   \n",
            "1        Albania       4880     144       2745    1991        117           6   \n",
            "2        Algeria      27973    1163      18837    7973        616           8   \n",
            "3        Andorra        907      52        803      52         10           0   \n",
            "4         Angola        950      41        242     667         18           1   \n",
            "\n",
            "   new_recovered  deaths_100_cases  recovered_100_cases  deaths_100_recovered  \\\n",
            "0             18              3.50                69.49                  5.04   \n",
            "1             63              2.95                56.25                  5.25   \n",
            "2            749              4.16                67.34                  6.17   \n",
            "3              0              5.73                88.53                  6.48   \n",
            "4              0              4.32                25.47                 16.94   \n",
            "\n",
            "   confirmed_last_week  week_change  week_perc_increase             who_region  \n",
            "0                35526          737                2.07  Eastern Mediterranean  \n",
            "1                 4171          709               17.00                 Europe  \n",
            "2                23691         4282               18.07                 Africa  \n",
            "3                  884           23                2.60                 Europe  \n",
            "4                  749          201               26.84                 Africa  \n",
            "\n",
            "Se generó el archivo  /content/parquet_pandas/country_wise_latest.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos la Clase CovidToParquet"
      ],
      "metadata": {
        "id": "PFkigZmBifh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase \"CovidToParquet\" realizará la lectura del archivo \"covid_19_clean_complete.csv\" y lo escribira en formato parquet, considerando el tipo de cada campo"
      ],
      "metadata": {
        "id": "74jbpglEimfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CovidToParquet:\n",
        "    def __init__(self, input_file, output_file):\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "    def read_csv(self):\n",
        "        # Leer el archivo CSV con tipos de campo especificados\n",
        "        dtype = {\n",
        "            'province_state': str,\n",
        "            'country_region': str,\n",
        "            'lat': float,\n",
        "            'long': float,\n",
        "            'date': str,\n",
        "            'confirmed': int,\n",
        "            'deaths': int,\n",
        "            'recovered': int,\n",
        "            'active': int,\n",
        "            'who_region': str,\n",
        "        }\n",
        "\n",
        "        columns = ['province_state','country_region','lat','long','date','confirmed','deaths','recovered','active','who_region']\n",
        "        df = pd.read_csv(self.input_file,header=None, names=columns, skiprows=1, dtype=dtype)\n",
        "        return df\n",
        "    # Se define un método convert_to_parquet() que toma un DataFrame df como entrada y escribe el DataFrame en formato Parquet.\n",
        "    def convert_to_parquet(self, df):\n",
        "        # Escribir el DataFrame en formato Parquet\n",
        "        df.to_parquet(self.output_file)\n",
        "        print(\"\\nSe generó el archivo \", self.output_file)\n",
        "\n",
        "    #realiza el proceso completo de conversión. obtiene el dataframe del método read_csv, muestra los tipos de datos de las columnas y las primeras 5 filas del DataFrame, y luego llama al método convert_to_parquet() para convertir el DataFrame a formato Parquet.\n",
        "    def convert(self):\n",
        "        df = self.read_csv()\n",
        "        print(df.dtypes, '\\n')\n",
        "        print(df.head(5))\n",
        "        self.convert_to_parquet(df)"
      ],
      "metadata": {
        "id": "Y-iGmkwFJPp2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea la instancia del convertidor y realizar la conversión"
      ],
      "metadata": {
        "id": "7gxmnhqmr2ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid = CovidToParquet(\"/content/kaggle/covid_19_clean_complete.csv\", \"/content/parquet_pandas/covid_19_clean_complete.parquet\")\n",
        "covid.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMY5Opf1r3ed",
        "outputId": "3c105c52-9c1a-430d-dd40-bcf442e5ea3a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "province_state     object\n",
            "country_region     object\n",
            "lat               float64\n",
            "long              float64\n",
            "date               object\n",
            "confirmed           int64\n",
            "deaths              int64\n",
            "recovered           int64\n",
            "active              int64\n",
            "who_region         object\n",
            "dtype: object \n",
            "\n",
            "  province_state country_region       lat       long        date  confirmed  \\\n",
            "0            NaN    Afghanistan  33.93911  67.709953  2020-01-22          0   \n",
            "1            NaN        Albania  41.15330  20.168300  2020-01-22          0   \n",
            "2            NaN        Algeria  28.03390   1.659600  2020-01-22          0   \n",
            "3            NaN        Andorra  42.50630   1.521800  2020-01-22          0   \n",
            "4            NaN         Angola -11.20270  17.873900  2020-01-22          0   \n",
            "\n",
            "   deaths  recovered  active             who_region  \n",
            "0       0          0       0  Eastern Mediterranean  \n",
            "1       0          0       0                 Europe  \n",
            "2       0          0       0                 Africa  \n",
            "3       0          0       0                 Europe  \n",
            "4       0          0       0                 Africa  \n",
            "\n",
            "Se generó el archivo  /content/parquet_pandas/covid_19_clean_complete.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos la Clase DayWiseToParquet"
      ],
      "metadata": {
        "id": "QjAN5mj1sm8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase \"DayWiseToParquet\" realizará la lectura del archivo \"day_wise.csv\" y lo escribira en formato parquet, considerando el tipo de cada campo"
      ],
      "metadata": {
        "id": "f6KiHbjnsp4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DayWiseToParquet:\n",
        "    def __init__(self, input_file, output_file):\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "    def read_csv(self):\n",
        "        # Leer el archivo CSV con tipos de campo especificados\n",
        "        dtype = {\n",
        "            'date': str,\n",
        "            'confirmed': int,\n",
        "            'deaths': int,\n",
        "            'recovered': int,\n",
        "            'active': int,\n",
        "            'new_cases': int,\n",
        "            'new_deaths': int,\n",
        "            'new_recovered': int,\n",
        "            'deaths_100_cases': float,\n",
        "            'recovered_100_cases': float,\n",
        "            'deaths_100_recovered': float,\n",
        "            'nro_countries': int,\n",
        "        }\n",
        "\n",
        "        columns = ['date','confirmed','deaths','recovered','active','new_cases','new_deaths','new_recovered','deaths_100_cases','recovered_100_cases','deaths_100_recovered','nro_countries']\n",
        "        df = pd.read_csv(self.input_file,header=None, names=columns, skiprows=1, dtype=dtype)\n",
        "        return df\n",
        "\n",
        "    # Se define un método convert_to_parquet() que toma un DataFrame df como entrada y escribe el DataFrame en formato Parquet.\n",
        "    def convert_to_parquet(self, df):\n",
        "        # Escribir el DataFrame en formato Parquet\n",
        "        df.to_parquet(self.output_file)\n",
        "        print(\"\\nSe generó el archivo \", self.output_file)\n",
        "\n",
        "    #realiza el proceso completo de conversión. obtiene el dataframe del método read_csv, muestra los tipos de datos de las columnas y las primeras 5 filas del DataFrame, y luego llama al método convert_to_parquet() para convertir el DataFrame a formato Parquet.\n",
        "    def convert(self):\n",
        "        df = self.read_csv()\n",
        "        print(df.dtypes, '\\n')\n",
        "        print(df.head(5))\n",
        "        self.convert_to_parquet(df)"
      ],
      "metadata": {
        "id": "UIQYGT26O452"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea la instancia del convertidor y realizar la conversión"
      ],
      "metadata": {
        "id": "JMv-YlfuswGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "day_wise = DayWiseToParquet(\"/content/kaggle/day_wise.csv\", \"/content/parquet_pandas/day_wise.parquet\")\n",
        "day_wise.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyqWviOUswc8",
        "outputId": "08ca4015-129f-4715-8c47-338bced51e60"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date                     object\n",
            "confirmed                 int64\n",
            "deaths                    int64\n",
            "recovered                 int64\n",
            "active                    int64\n",
            "new_cases                 int64\n",
            "new_deaths                int64\n",
            "new_recovered             int64\n",
            "deaths_100_cases        float64\n",
            "recovered_100_cases     float64\n",
            "deaths_100_recovered    float64\n",
            "nro_countries             int64\n",
            "dtype: object \n",
            "\n",
            "         date  confirmed  deaths  recovered  active  new_cases  new_deaths  \\\n",
            "0  2020-01-22        555      17         28     510          0           0   \n",
            "1  2020-01-23        654      18         30     606         99           1   \n",
            "2  2020-01-24        941      26         36     879        287           8   \n",
            "3  2020-01-25       1434      42         39    1353        493          16   \n",
            "4  2020-01-26       2118      56         52    2010        684          14   \n",
            "\n",
            "   new_recovered  deaths_100_cases  recovered_100_cases  deaths_100_recovered  \\\n",
            "0              0              3.06                 5.05                 60.71   \n",
            "1              2              2.75                 4.59                 60.00   \n",
            "2              6              2.76                 3.83                 72.22   \n",
            "3              3              2.93                 2.72                107.69   \n",
            "4             13              2.64                 2.46                107.69   \n",
            "\n",
            "   nro_countries  \n",
            "0              6  \n",
            "1              8  \n",
            "2              9  \n",
            "3             11  \n",
            "4             13  \n",
            "\n",
            "Se generó el archivo  /content/parquet_pandas/day_wise.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos la Clase FullGroupedToParquet"
      ],
      "metadata": {
        "id": "0r3VpL69sw_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase \"FullGroupedToParquet\" realizará la lectura del archivo \"full_grouped.csv\" y lo escribira en formato parquet, considerando el tipo de cada campo"
      ],
      "metadata": {
        "id": "thnzW8xQtCBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullGroupedToParquet:\n",
        "    def __init__(self, input_file, output_file):\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "    def read_csv(self):\n",
        "        # Leer el archivo CSV con tipos de campo especificados\n",
        "        dtype = {\n",
        "            'date': str,\n",
        "            'country_region': str,\n",
        "            'confirmed': int,\n",
        "            'deaths': int,\n",
        "            'recovered': int,\n",
        "            'active': int,\n",
        "            'new_cases': int,\n",
        "            'new_deaths': int,\n",
        "            'new_recovered': int,\n",
        "            'who_region': str,\n",
        "        }\n",
        "\n",
        "        columns = ['date','country_region','confirmed','deaths','recovered','active','new_cases','new_deaths','new_recovered','who_region']\n",
        "        df = pd.read_csv(self.input_file,header=None, names=columns, skiprows=1, dtype=dtype)\n",
        "        return df\n",
        "\n",
        "    # Se define un método convert_to_parquet() que toma un DataFrame df como entrada y escribe el DataFrame en formato Parquet.\n",
        "    def convert_to_parquet(self, df):\n",
        "        # Escribir el DataFrame en formato Parquet\n",
        "        df.to_parquet(self.output_file)\n",
        "        print(\"\\nSe generó el archivo \", self.output_file)\n",
        "\n",
        "    #realiza el proceso completo de conversión. obtiene el dataframe del método read_csv, muestra los tipos de datos de las columnas y las primeras 5 filas del DataFrame, y luego llama al método convert_to_parquet() para convertir el DataFrame a formato Parquet.\n",
        "    def convert(self):\n",
        "        df = self.read_csv()\n",
        "        print(df.dtypes, '\\n')\n",
        "        print(df.head(5))\n",
        "        self.convert_to_parquet(df)"
      ],
      "metadata": {
        "id": "Yo0bxqVeP-87"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea la instancia del convertidor y realizar la conversión"
      ],
      "metadata": {
        "id": "C3qljto4tHKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_grouped = FullGroupedToParquet(\"/content/kaggle/full_grouped.csv\", \"/content/parquet_pandas/full_grouped.parquet\")\n",
        "full_grouped.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ4V2OX9tHWM",
        "outputId": "5009a773-63f6-4f31-f8f9-81da0c71a8bb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date              object\n",
            "country_region    object\n",
            "confirmed          int64\n",
            "deaths             int64\n",
            "recovered          int64\n",
            "active             int64\n",
            "new_cases          int64\n",
            "new_deaths         int64\n",
            "new_recovered      int64\n",
            "who_region        object\n",
            "dtype: object \n",
            "\n",
            "         date country_region  confirmed  deaths  recovered  active  new_cases  \\\n",
            "0  2020-01-22    Afghanistan          0       0          0       0          0   \n",
            "1  2020-01-22        Albania          0       0          0       0          0   \n",
            "2  2020-01-22        Algeria          0       0          0       0          0   \n",
            "3  2020-01-22        Andorra          0       0          0       0          0   \n",
            "4  2020-01-22         Angola          0       0          0       0          0   \n",
            "\n",
            "   new_deaths  new_recovered             who_region  \n",
            "0           0              0  Eastern Mediterranean  \n",
            "1           0              0                 Europe  \n",
            "2           0              0                 Africa  \n",
            "3           0              0                 Europe  \n",
            "4           0              0                 Africa  \n",
            "\n",
            "Se generó el archivo  /content/parquet_pandas/full_grouped.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos la Clase USACountyToParquet"
      ],
      "metadata": {
        "id": "mlIzFPCftHyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase \"USACountyToParquet\" realizará la lectura del archivo \"usa_county_wise.csv\" y lo escribira en formato parquet, considerando el tipo de cada campo"
      ],
      "metadata": {
        "id": "vKC-0_xOtYIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class USACountyToParquet:\n",
        "    def __init__(self, input_file, output_file):\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "    def read_csv(self):\n",
        "        # Leer el archivo CSV con tipos de campo especificados\n",
        "        dtype = {\n",
        "            'uid': int,\n",
        "            'iso2': str,\n",
        "            'iso3': str,\n",
        "            'code3': int,\n",
        "            'fips': float,\n",
        "            'admin2': str,\n",
        "            'province_state': str,\n",
        "            'country_region': str,\n",
        "            'lat': float,\n",
        "            'long': float,\n",
        "            'combined_key': str,\n",
        "            'date': str,\n",
        "            'confirmed': int,\n",
        "            'deaths': int,\n",
        "        }\n",
        "\n",
        "        columns = ['uid','iso2','iso3','code3','fips','admin2','province_state','country_region','lat','long','combined_key','date','confirmed','deaths']\n",
        "        df = pd.read_csv(self.input_file,header=None, names=columns, skiprows=1, dtype=dtype)\n",
        "        return df\n",
        "\n",
        "    # Se define un método convert_to_parquet() que toma un DataFrame df como entrada y escribe el DataFrame en formato Parquet.\n",
        "    def convert_to_parquet(self, df):\n",
        "        # Escribir el DataFrame en formato Parquet\n",
        "        df.to_parquet(self.output_file)\n",
        "        print(\"\\nSe generó el archivo \", self.output_file)\n",
        "\n",
        "    #realiza el proceso completo de conversión. obtiene el dataframe del método read_csv, muestra los tipos de datos de las columnas y las primeras 5 filas del DataFrame, y luego llama al método convert_to_parquet() para convertir el DataFrame a formato Parquet.\n",
        "    def convert(self):\n",
        "        df = self.read_csv()\n",
        "        print(df.dtypes, '\\n')\n",
        "        print(df.head(5))\n",
        "        self.convert_to_parquet(df)"
      ],
      "metadata": {
        "id": "lsQXmVRJRMIX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea la instancia del convertidor y realizar la conversión"
      ],
      "metadata": {
        "id": "8eZ8ZBPztiOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usa_county = USACountyToParquet(\"/content/kaggle/usa_county_wise.csv\", \"/content/parquet_pandas/usa_county_wise.parquet\")\n",
        "usa_county.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLtA8bWttiUi",
        "outputId": "9832bb64-8096-496e-cf97-01f31b22993d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uid                 int64\n",
            "iso2               object\n",
            "iso3               object\n",
            "code3               int64\n",
            "fips              float64\n",
            "admin2             object\n",
            "province_state     object\n",
            "country_region     object\n",
            "lat               float64\n",
            "long              float64\n",
            "combined_key       object\n",
            "date               object\n",
            "confirmed           int64\n",
            "deaths              int64\n",
            "dtype: object \n",
            "\n",
            "        uid iso2 iso3  code3     fips    admin2            province_state  \\\n",
            "0        16   AS  ASM     16     60.0       NaN            American Samoa   \n",
            "1       316   GU  GUM    316     66.0       NaN                      Guam   \n",
            "2       580   MP  MNP    580     69.0       NaN  Northern Mariana Islands   \n",
            "3  63072001   PR  PRI    630  72001.0  Adjuntas               Puerto Rico   \n",
            "4  63072003   PR  PRI    630  72003.0    Aguada               Puerto Rico   \n",
            "\n",
            "  country_region        lat        long                  combined_key  \\\n",
            "0             US -14.271000 -170.132000            American Samoa, US   \n",
            "1             US  13.444300  144.793700                      Guam, US   \n",
            "2             US  15.097900  145.673900  Northern Mariana Islands, US   \n",
            "3             US  18.180117  -66.754367     Adjuntas, Puerto Rico, US   \n",
            "4             US  18.360255  -67.175131       Aguada, Puerto Rico, US   \n",
            "\n",
            "      date  confirmed  deaths  \n",
            "0  1/22/20          0       0  \n",
            "1  1/22/20          0       0  \n",
            "2  1/22/20          0       0  \n",
            "3  1/22/20          0       0  \n",
            "4  1/22/20          0       0  \n",
            "\n",
            "Se generó el archivo  /content/parquet_pandas/usa_county_wise.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos la Clase WorldometerToParquet"
      ],
      "metadata": {
        "id": "cgsdBymNtiZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase \"WorldometerToParquet\" realizará la lectura del archivo \"worldometer_data.csv\" y lo escribira en formato parquet, considerando el tipo de cada campo"
      ],
      "metadata": {
        "id": "6NYXYdBRtzzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WorldometerToParquet:\n",
        "    def __init__(self, input_file, output_file):\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "    def read_csv(self):\n",
        "        # Leer el archivo CSV con tipos de campo especificados\n",
        "        dtype = {\n",
        "            'country_region': str,\n",
        "            'continent': str,\n",
        "            'population': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'totalcases': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'newcases': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'totaldeaths': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'newdeaths': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'totalrecovered': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'newrecovered': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'activecases': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'serious_critical': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'tot_cases_1m_pop': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'deaths_1m_pop': float,\n",
        "            'totaltests': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'tests_1m_pop': 'Int64',  # Utilizamos 'Int64' para permitir valores nulos en campos enteros\n",
        "            'who_region': str,\n",
        "        }\n",
        "\n",
        "        na_values = ['', 'NA', 'N/A', 'null'] \n",
        "        columns = ['country_region','continent','population','totalcases','newcases','totaldeaths','newdeaths','totalrecovered','newrecovered','activecases','serious_critical','tot_cases_1m_pop','deaths_1m_pop','totaltests','tests_1m_pop','who_region']\n",
        "        \n",
        "        df = pd.read_csv(self.input_file,header=None, names=columns, skiprows=1, dtype=dtype, na_values=na_values)\n",
        "        return df\n",
        "\n",
        "    # Se define un método convert_to_parquet() que toma un DataFrame df como entrada y escribe el DataFrame en formato Parquet.\n",
        "    def convert_to_parquet(self, df):\n",
        "        # Escribir el DataFrame en formato Parquet\n",
        "        df.to_parquet(self.output_file)\n",
        "        print(\"\\nSe generó el archivo \", self.output_file)\n",
        "\n",
        "    #realiza el proceso completo de conversión. obtiene el dataframe del método read_csv, muestra los tipos de datos de las columnas y las primeras 5 filas del DataFrame, y luego llama al método convert_to_parquet() para convertir el DataFrame a formato Parquet.\n",
        "    def convert(self):\n",
        "        df = self.read_csv()\n",
        "        print(df.dtypes, '\\n')\n",
        "        print(df.head(5))\n",
        "        self.convert_to_parquet(df)"
      ],
      "metadata": {
        "id": "8NBWLf0qR61u"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea la instancia del convertidor y realizar la conversión"
      ],
      "metadata": {
        "id": "Df_PvkWpt-aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "worldometer = WorldometerToParquet(\"/content/kaggle/worldometer_data.csv\", \"/content/parquet_pandas/worldometer_data.parquet\")\n",
        "worldometer.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXoebsg0uDaV",
        "outputId": "e54b29c3-6706-4114-c36e-0eab506817eb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "country_region       object\n",
            "continent            object\n",
            "population            Int64\n",
            "totalcases            Int64\n",
            "newcases              Int64\n",
            "totaldeaths           Int64\n",
            "newdeaths             Int64\n",
            "totalrecovered        Int64\n",
            "newrecovered          Int64\n",
            "activecases           Int64\n",
            "serious_critical      Int64\n",
            "tot_cases_1m_pop      Int64\n",
            "deaths_1m_pop       float64\n",
            "totaltests            Int64\n",
            "tests_1m_pop          Int64\n",
            "who_region           object\n",
            "dtype: object \n",
            "\n",
            "  country_region      continent  population  totalcases  newcases  \\\n",
            "0            USA  North America   331198130     5032179      <NA>   \n",
            "1         Brazil  South America   212710692     2917562      <NA>   \n",
            "2          India           Asia  1381344997     2025409      <NA>   \n",
            "3         Russia         Europe   145940924      871894      <NA>   \n",
            "4   South Africa         Africa    59381566      538184      <NA>   \n",
            "\n",
            "   totaldeaths  newdeaths  totalrecovered  newrecovered  activecases  \\\n",
            "0       162804       <NA>         2576668          <NA>      2292707   \n",
            "1        98644       <NA>         2047660          <NA>       771258   \n",
            "2        41638       <NA>         1377384          <NA>       606387   \n",
            "3        14606       <NA>          676357          <NA>       180931   \n",
            "4         9604       <NA>          387316          <NA>       141264   \n",
            "\n",
            "   serious_critical  tot_cases_1m_pop  deaths_1m_pop  totaltests  \\\n",
            "0             18296             15194          492.0    63139605   \n",
            "1              8318             13716          464.0    13206188   \n",
            "2              8944              1466           30.0    22149351   \n",
            "3              2300              5974          100.0    29716907   \n",
            "4               539              9063          162.0     3149807   \n",
            "\n",
            "   tests_1m_pop      who_region  \n",
            "0        190640        Americas  \n",
            "1         62085        Americas  \n",
            "2         16035  South-EastAsia  \n",
            "3        203623          Europe  \n",
            "4         53044          Africa  \n",
            "\n",
            "Se generó el archivo  /content/parquet_pandas/worldometer_data.parquet\n"
          ]
        }
      ]
    }
  ]
}